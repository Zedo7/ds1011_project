{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# src/models ? Transformer (minimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Minimal time-series Transformer with pluggable PEs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport torch, torch.nn as nn\nclass TSTransformer(nn.Module):\n    def __init__(self, d_model=512, n_heads=8, n_layers=6, dropout=0.1, pe='rope'):\n        super().__init__()\n        self.emb = nn.Linear(1, d_model)\n        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n        self.head = nn.Linear(d_model, 1); self.pe_type = pe\n    def forward(self, x):\n        h = self.emb(x)   # (B,L,1)->(B,L,D)\n        # TODO: apply positional encoding to h\n        h = self.encoder(h)\n        return self.head(h[:, -1:, :])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}